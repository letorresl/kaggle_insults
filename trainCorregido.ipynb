{
 "metadata": {
  "name": "trainCorregido"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load train.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "from sklearn.metrics import roc_auc_score, make_scorer\n",
      "from multiprocessing import cpu_count\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "#from sklearn.cross_validation import train_test_split\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "#from sklearn.cross_validation import cross_val_score\n",
      "#from sklearn.svm import LinearSVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "#from sklearn.pipeline import Pipeline\n",
      "import scipy\n",
      "#from sklearn.ensemble import RandomForestClassifier\n",
      "from IPython.core.debugger import Tracer\n",
      "\n",
      "tracer = Tracer()\n",
      "\n",
      "inicio = time.time()\n",
      "\n",
      "def load_data():\n",
      "    print(\"loading\")\n",
      "    comments = []\n",
      "    dates = []\n",
      "    labels = []\n",
      "\n",
      "    with open(\"train.csv\") as f:\n",
      "        f.readline()\n",
      "        for line in f:\n",
      "            splitstring = line.split(',')\n",
      "            labels.append(splitstring[0])\n",
      "            dates.append(splitstring[1][:-1])\n",
      "            comment = \",\".join(splitstring[2:])\n",
      "            comment = comment.strip().strip('\"')\n",
      "            comment.replace('_', ' ')\n",
      "            comments.append(comment)\n",
      "    labels = np.array(labels, dtype=np.int)\n",
      "    dates = np.array(dates)\n",
      "    return comments, dates, labels\n",
      "\n",
      "\n",
      "def load_test():\n",
      "    print(\"loading test set\")\n",
      "    comments = []\n",
      "    dates = []\n",
      "\n",
      "    with open(\"test.csv\") as f:\n",
      "        f.readline()\n",
      "        for line in f:\n",
      "            splitstring = line.split(',')\n",
      "            dates.append(splitstring[0][:-1])\n",
      "            comment = \",\".join(splitstring[1:])\n",
      "            comment = comment.strip().strip('\"')\n",
      "            comment.replace('_', ' ')\n",
      "            comments.append(comment)\n",
      "    dates = np.array(dates)\n",
      "    return comments, dates\n",
      "\n",
      "\n",
      "def write_test(labels, fname=\"test_prediction.csv\"):\n",
      "    with open(\"test.csv\") as f:\n",
      "        with open(fname, 'w') as fw:\n",
      "            fw.write(f.readline())\n",
      "            for label, line in zip(labels, f):\n",
      "                fw.write(\"%f,\" % label)\n",
      "                fw.write(line)\n",
      "\n",
      "\n",
      "def grid_search():\n",
      "    comments, dates, labels = load_data()\n",
      "\n",
      "    print(\"vectorizing\")\n",
      "    countvect = CountVectorizer(max_n=2)\n",
      "    countvect_char = CountVectorizer(max_n=6, analyzer=\"char\")\n",
      "    #countvect = TfidfVectorizer()\n",
      "\n",
      "    counts = countvect.fit_transform(comments)\n",
      "    counts_char = countvect_char.fit_transform(comments)\n",
      "    features = scipy.sparse.hstack([counts, counts_char])\n",
      "    tracer()\n",
      "    #counts_array = counts.toarray()\n",
      "    #indicators = sparse.csr_matrix((counts_array > 0).astype(np.int))\n",
      "\n",
      "    #X_train, X_test, y_train, y_test = train_test_split(counts, labels,\n",
      "                                                        #test_size=0.5)\n",
      "    #inds = np.random.permutation(len(labels))\n",
      "    #n_samples = len(labels)\n",
      "    #print(\"training\")\n",
      "    #param_grid = dict(logr__C=2. ** np.arange(-6, 4),\n",
      "            #logr__penalty=['l1', 'l2'],\n",
      "            #vect__max_n=np.arange(1, 4), vect__lowercase=[True, False])\n",
      "    param_grid = dict(C=2. ** np.arange(-3, 4),\n",
      "            penalty=['l1', 'l2'])\n",
      "    #clf = LinearSVC(tol=1e-8, penalty='l1', dual=False, C=0.5)\n",
      "    clf = LogisticRegression(tol=1e-8, penalty='l1', C=2)\n",
      "    #pipeline = Pipeline([('vect', countvect), ('logr', clf)])\n",
      "    #feature_selector.fit(comments, labels)\n",
      "    #features = feature_selector.transform(comments).toarray()\n",
      "\n",
      "    #clf = NearestCentroid()\n",
      "\n",
      "    #param_grid = dict(max_depth=np.arange(1, 20), max_features=['sqrt', 'log2', None])\n",
      "    #clf = RandomForestClassifier(n_estimators=10)\n",
      "\n",
      "    puntuador = make_scorer(roc_auc_score, greater_is_better=True)\n",
      "    numerocpus = cpu_count()\n",
      "    grid = GridSearchCV(clf, cv=5, param_grid=param_grid, verbose=4, scoring=puntuador,\n",
      "            n_jobs=numerocpus)\n",
      "    #print(cross_val_score(clf, counts, labels, cv=3))\n",
      "\n",
      "    grid.fit(features, labels)\n",
      "    print(grid.best_score_)\n",
      "    print(grid.best_params_)\n",
      "    #clf.fit(X_train, y_train)\n",
      "    #print(clf.score(X_test, y_test))\n",
      "    #tracer()\n",
      "    comments_test, dates_test = load_test()\n",
      "    prob_pred = grid.best_estimator_.predict_proba(comments_test)\n",
      "    write_test(prob_pred[:, 1])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    grid_search()\n",
      "\n",
      "\n",
      "final = time.time()\n",
      "tiempoTotal = final - inicio\n",
      "print tiempoTotal\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}