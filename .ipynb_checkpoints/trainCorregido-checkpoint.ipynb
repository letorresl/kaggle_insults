{
 "metadata": {
  "name": "trainCorregido"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load train.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "from sklearn.metrics import roc_auc_score, make_scorer\n",
      "from multiprocessing import cpu_count\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "#from sklearn.cross_validation import train_test_split\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "#from sklearn.cross_validation import cross_val_score\n",
      "#from sklearn.svm import LinearSVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.pipeline import Pipeline\n",
      "#from scipy import sparse\n",
      "#from sklearn.ensemble import RandomForestClassifier\n",
      "from IPython.core.debugger import Tracer\n",
      "\n",
      "\n",
      "def load_data():\n",
      "    print(\"loading\")\n",
      "    comments = []\n",
      "    dates = []\n",
      "    labels = []\n",
      "\n",
      "    with open(\"train.csv\") as f:\n",
      "        f.readline()\n",
      "        for line in f:\n",
      "            splitstring = line.split(',')\n",
      "            labels.append(splitstring[0])\n",
      "            dates.append(splitstring[1][:-1])\n",
      "            comment = \",\".join(splitstring[2:])\n",
      "            comment = comment.strip().strip('\"')\n",
      "            comment.replace('_', ' ')\n",
      "            comments.append(comment)\n",
      "    labels = np.array(labels, dtype=np.int)\n",
      "    dates = np.array(dates)\n",
      "    return comments, dates, labels\n",
      "\n",
      "\n",
      "def load_test():\n",
      "    print(\"loading test set\")\n",
      "    comments = []\n",
      "    dates = []\n",
      "\n",
      "    with open(\"test.csv\") as f:\n",
      "        f.readline()\n",
      "        for line in f:\n",
      "            splitstring = line.split(',')\n",
      "            dates.append(splitstring[0][:-1])\n",
      "            comment = \",\".join(splitstring[1:])\n",
      "            comment = comment.strip().strip('\"')\n",
      "            comment.replace('_', ' ')\n",
      "            comments.append(comment)\n",
      "    dates = np.array(dates)\n",
      "    return comments, dates\n",
      "\n",
      "\n",
      "def write_test(labels, fname=\"test_prediction.csv\"):\n",
      "    with open(\"test.csv\") as f:\n",
      "        with open(fname, 'w') as fw:\n",
      "            fw.write(f.readline())\n",
      "            for label, line in zip(labels, f):\n",
      "                fw.write(\"%f,\" % label)\n",
      "                fw.write(line)\n",
      "\n",
      "\n",
      "def grid_search():\n",
      "    comments, dates, labels = load_data()\n",
      "\n",
      "    print(\"vecorizing\")\n",
      "    countvect = CountVectorizer(ngram_range=(1,3))\n",
      "    #countvect = TfidfVectorizer()\n",
      "\n",
      "    #counts = countvect.fit_transform(comments)\n",
      "    #counts_array = counts.toarray()\n",
      "    #indicators = sparse.csr_matrix((counts_array > 0).astype(np.int))\n",
      "\n",
      "    #X_train, X_test, y_train, y_test = train_test_split(counts, labels,\n",
      "                                                        #test_size=0.5)\n",
      "    #inds = np.random.permutation(len(labels))\n",
      "    #n_samples = len(labels)\n",
      "    #print(\"training\")\n",
      "    param_grid = dict(logr__C=2. ** np.arange(-6, 4),\n",
      "            logr__penalty=['l1', 'l2'],\n",
      "            vect__ngram_range=[(1,1),(1,2),(1,3),(1, 4)], vect__lowercase=[True, False])\n",
      "    #clf = LinearSVC(tol=1e-8, penalty='l1', dual=False, C=0.5)\n",
      "    clf = LogisticRegression(tol=1e-8)\n",
      "    pipeline = Pipeline([('vect', countvect), ('logr', clf)])\n",
      "\n",
      "    #clf = NearestCentroid()\n",
      "\n",
      "    #param_grid = dict(max_depth=np.arange(1, 10))\n",
      "    #clf = RandomForestClassifier(n_estimators=10)\n",
      "\n",
      "    numerocpus = cpu_count()\n",
      "    puntuador = make_scorer(roc_auc_score, greater_is_better = True)\n",
      "    grid = GridSearchCV(pipeline, cv=2, param_grid=param_grid, verbose=4, scoring=puntuador,\n",
      "            n_jobs=numerocpus)\n",
      "    #print(cross_val_score(clf, counts, labels, cv=3))\n",
      "\n",
      "    grid.fit(comments, labels)\n",
      "    #clf.fit(X_train, y_train)\n",
      "    #print(clf.score(X_test, y_test))\n",
      "    comments_test, dates_test = load_test()\n",
      "    #counts_test = countvect.transform(comments_test)\n",
      "    prob_pred = grid.best_estimator_.predict_proba(comments_test)\n",
      "    print grid.best_estimator_\n",
      "    write_test(prob_pred[:, 1])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    inicio = time.time()\n",
      "    grid_search()\n",
      "    #tracer = Tracer()\n",
      "    final = time.time()\n",
      "    tiempoTotal = final - inicio\n",
      "    print tiempoTotal\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading\n",
        "vecorizing\n",
        "Fitting 2 folds for each of 160 candidates, totalling 320 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=4)]: Done   1 jobs       | elapsed:    0.5s\n",
        "[Parallel(n_jobs=4)]: Done  25 jobs       | elapsed:   14.8s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=4)]: Done  98 jobs       | elapsed:  1.1min\n",
        "[Parallel(n_jobs=4)]: Done 221 jobs       | elapsed:  2.8min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading test set\n",
        "472.906394005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=4)]: Done 320 out of 320 | elapsed:  7.9min finished\n"
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}